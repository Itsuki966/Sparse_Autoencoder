{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462cc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sae_model import SparseAutoencoder\n",
    "from activation_utils import get_llm_activations_residual_stream\n",
    "from sae_trainer import extract_activations, create_data_loader, train_sparse_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_activationsの変数設定\n",
    "llm_model_name = \"bert-base-uncased\"    # 使用するLLMモデル名    \n",
    "texts = [\n",
    "    # \"The quick brown fox jumps over the lazy dog.\",\n",
    "    # \"A large language model can process and generate text.\",\n",
    "    # \"Cats enjoy sleeping in warm, sunny spots.\",\n",
    "    # \"This is a sample text for testing the Sparse Autoencoder.\",\n",
    "    # \"PyTorch is a widely used deep learning framework.\" ,\n",
    "    \"A: Why are you carrying that wet umbrella? B: Because it started to rain on my way here, I had to use it.\",\n",
    "    \"A: You seem very tired today. B: I am. I stayed up late to finish my report, so I didn't get much sleep.\",\n",
    "    \"A: The room is so bright all of a sudden. What happened? B: I just pushed the main switch, which caused all the lights to turn on.\",\n",
    "    \"A: I heard you passed the difficult exam. Congratulations! B: Thank you! I studied hard every day for it, so I'm glad my effort paid off.\"\n",
    "    ]  # 使用するテキスト\n",
    "target_layer_idx = 4    # 抽出するLLMの層インデックス\n",
    "num_samples_for_training = 4  # 訓練に使用するサンプル数\n",
    "\n",
    "# create_data_loaderの変数設定\n",
    "batch_size = 256  # バッチサイズ\n",
    "\n",
    "# train_sparse_autoencoderの変数設定\n",
    "num_epochs = 200  # 訓練エポック数\n",
    "sae_l1_coeff = 1 # スパース性の度合いを調整する係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7240cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModel.from_pretrained(llm_model_name)\n",
    "training_texts = [texts[i] for i in range(num_samples_for_training)]\n",
    "\n",
    "activations, activations_dict = get_llm_activations_residual_stream(\n",
    "    llm_model, tokenizer, training_texts, target_layer_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0937d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "----------LLM・SAEの情報----------\n",
      "LLMモデル名: bert-base-uncased\n",
      "LLMの活性化ベクトルの次元数: 768, SAEの特徴次元: 844\n",
      "----------------------------------\n",
      "\n",
      "Starting SAE training for 200 epochs...\n",
      "Epoch 1/200, Total Loss: 0.7702, Recon Loss: 0.5991, Sparse Loss: 0.1710\n",
      "Epoch 2/200, Total Loss: 0.6589, Recon Loss: 0.5427, Sparse Loss: 0.1162\n",
      "Epoch 3/200, Total Loss: 0.6003, Recon Loss: 0.5202, Sparse Loss: 0.0801\n",
      "Epoch 4/200, Total Loss: 0.5656, Recon Loss: 0.5071, Sparse Loss: 0.0585\n",
      "Epoch 5/200, Total Loss: 0.5423, Recon Loss: 0.4945, Sparse Loss: 0.0478\n",
      "Epoch 6/200, Total Loss: 0.5232, Recon Loss: 0.4788, Sparse Loss: 0.0444\n",
      "Epoch 7/200, Total Loss: 0.5055, Recon Loss: 0.4595, Sparse Loss: 0.0460\n",
      "Epoch 8/200, Total Loss: 0.4889, Recon Loss: 0.4383, Sparse Loss: 0.0506\n",
      "Epoch 9/200, Total Loss: 0.4736, Recon Loss: 0.4169, Sparse Loss: 0.0567\n",
      "Epoch 10/200, Total Loss: 0.4586, Recon Loss: 0.3958, Sparse Loss: 0.0627\n",
      "Epoch 11/200, Total Loss: 0.4434, Recon Loss: 0.3753, Sparse Loss: 0.0681\n",
      "Epoch 12/200, Total Loss: 0.4280, Recon Loss: 0.3559, Sparse Loss: 0.0722\n",
      "Epoch 13/200, Total Loss: 0.4126, Recon Loss: 0.3377, Sparse Loss: 0.0748\n",
      "Epoch 14/200, Total Loss: 0.3971, Recon Loss: 0.3208, Sparse Loss: 0.0763\n",
      "Epoch 15/200, Total Loss: 0.3818, Recon Loss: 0.3048, Sparse Loss: 0.0770\n",
      "Epoch 16/200, Total Loss: 0.3668, Recon Loss: 0.2894, Sparse Loss: 0.0774\n",
      "Epoch 17/200, Total Loss: 0.3521, Recon Loss: 0.2742, Sparse Loss: 0.0779\n",
      "Epoch 18/200, Total Loss: 0.3378, Recon Loss: 0.2590, Sparse Loss: 0.0787\n",
      "Epoch 19/200, Total Loss: 0.3240, Recon Loss: 0.2441, Sparse Loss: 0.0799\n",
      "Epoch 20/200, Total Loss: 0.3110, Recon Loss: 0.2295, Sparse Loss: 0.0814\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 21/200, Total Loss: 0.2986, Recon Loss: 0.2155, Sparse Loss: 0.0831\n",
      "Epoch 22/200, Total Loss: 0.2869, Recon Loss: 0.2022, Sparse Loss: 0.0847\n",
      "Epoch 23/200, Total Loss: 0.2756, Recon Loss: 0.1894, Sparse Loss: 0.0862\n",
      "Epoch 24/200, Total Loss: 0.2647, Recon Loss: 0.1774, Sparse Loss: 0.0873\n",
      "Epoch 25/200, Total Loss: 0.2545, Recon Loss: 0.1663, Sparse Loss: 0.0882\n",
      "Epoch 26/200, Total Loss: 0.2449, Recon Loss: 0.1561, Sparse Loss: 0.0888\n",
      "Epoch 27/200, Total Loss: 0.2360, Recon Loss: 0.1468, Sparse Loss: 0.0892\n",
      "Epoch 28/200, Total Loss: 0.2276, Recon Loss: 0.1382, Sparse Loss: 0.0894\n",
      "Epoch 29/200, Total Loss: 0.2197, Recon Loss: 0.1303, Sparse Loss: 0.0895\n",
      "Epoch 30/200, Total Loss: 0.2124, Recon Loss: 0.1230, Sparse Loss: 0.0894\n",
      "Epoch 31/200, Total Loss: 0.2055, Recon Loss: 0.1163, Sparse Loss: 0.0892\n",
      "Epoch 32/200, Total Loss: 0.1991, Recon Loss: 0.1103, Sparse Loss: 0.0888\n",
      "Epoch 33/200, Total Loss: 0.1932, Recon Loss: 0.1049, Sparse Loss: 0.0883\n",
      "Epoch 34/200, Total Loss: 0.1877, Recon Loss: 0.1000, Sparse Loss: 0.0877\n",
      "Epoch 35/200, Total Loss: 0.1825, Recon Loss: 0.0955, Sparse Loss: 0.0870\n",
      "Epoch 36/200, Total Loss: 0.1777, Recon Loss: 0.0914, Sparse Loss: 0.0863\n",
      "Epoch 37/200, Total Loss: 0.1732, Recon Loss: 0.0874, Sparse Loss: 0.0857\n",
      "Epoch 38/200, Total Loss: 0.1690, Recon Loss: 0.0837, Sparse Loss: 0.0853\n",
      "Epoch 39/200, Total Loss: 0.1650, Recon Loss: 0.0801, Sparse Loss: 0.0849\n",
      "Epoch 40/200, Total Loss: 0.1613, Recon Loss: 0.0766, Sparse Loss: 0.0847\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 41/200, Total Loss: 0.1579, Recon Loss: 0.0734, Sparse Loss: 0.0845\n",
      "Epoch 42/200, Total Loss: 0.1546, Recon Loss: 0.0704, Sparse Loss: 0.0842\n",
      "Epoch 43/200, Total Loss: 0.1515, Recon Loss: 0.0676, Sparse Loss: 0.0839\n",
      "Epoch 44/200, Total Loss: 0.1486, Recon Loss: 0.0651, Sparse Loss: 0.0835\n",
      "Epoch 45/200, Total Loss: 0.1459, Recon Loss: 0.0628, Sparse Loss: 0.0831\n",
      "Epoch 46/200, Total Loss: 0.1433, Recon Loss: 0.0607, Sparse Loss: 0.0826\n",
      "Epoch 47/200, Total Loss: 0.1408, Recon Loss: 0.0586, Sparse Loss: 0.0822\n",
      "Epoch 48/200, Total Loss: 0.1385, Recon Loss: 0.0566, Sparse Loss: 0.0819\n",
      "Epoch 49/200, Total Loss: 0.1362, Recon Loss: 0.0546, Sparse Loss: 0.0816\n",
      "Epoch 50/200, Total Loss: 0.1341, Recon Loss: 0.0527, Sparse Loss: 0.0814\n",
      "Epoch 51/200, Total Loss: 0.1321, Recon Loss: 0.0508, Sparse Loss: 0.0812\n",
      "Epoch 52/200, Total Loss: 0.1301, Recon Loss: 0.0491, Sparse Loss: 0.0811\n",
      "Epoch 53/200, Total Loss: 0.1282, Recon Loss: 0.0474, Sparse Loss: 0.0808\n",
      "Epoch 54/200, Total Loss: 0.1264, Recon Loss: 0.0459, Sparse Loss: 0.0806\n",
      "Epoch 55/200, Total Loss: 0.1247, Recon Loss: 0.0444, Sparse Loss: 0.0803\n",
      "Epoch 56/200, Total Loss: 0.1231, Recon Loss: 0.0431, Sparse Loss: 0.0800\n",
      "Epoch 57/200, Total Loss: 0.1215, Recon Loss: 0.0418, Sparse Loss: 0.0797\n",
      "Epoch 58/200, Total Loss: 0.1200, Recon Loss: 0.0405, Sparse Loss: 0.0794\n",
      "Epoch 59/200, Total Loss: 0.1185, Recon Loss: 0.0394, Sparse Loss: 0.0791\n",
      "Epoch 60/200, Total Loss: 0.1171, Recon Loss: 0.0383, Sparse Loss: 0.0788\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 61/200, Total Loss: 0.1157, Recon Loss: 0.0373, Sparse Loss: 0.0784\n",
      "Epoch 62/200, Total Loss: 0.1144, Recon Loss: 0.0363, Sparse Loss: 0.0781\n",
      "Epoch 63/200, Total Loss: 0.1131, Recon Loss: 0.0354, Sparse Loss: 0.0777\n",
      "Epoch 64/200, Total Loss: 0.1119, Recon Loss: 0.0346, Sparse Loss: 0.0773\n",
      "Epoch 65/200, Total Loss: 0.1107, Recon Loss: 0.0338, Sparse Loss: 0.0770\n",
      "Epoch 66/200, Total Loss: 0.1096, Recon Loss: 0.0330, Sparse Loss: 0.0766\n",
      "Epoch 67/200, Total Loss: 0.1085, Recon Loss: 0.0322, Sparse Loss: 0.0762\n",
      "Epoch 68/200, Total Loss: 0.1074, Recon Loss: 0.0316, Sparse Loss: 0.0758\n",
      "Epoch 69/200, Total Loss: 0.1063, Recon Loss: 0.0309, Sparse Loss: 0.0754\n",
      "Epoch 70/200, Total Loss: 0.1053, Recon Loss: 0.0303, Sparse Loss: 0.0750\n",
      "Epoch 71/200, Total Loss: 0.1043, Recon Loss: 0.0298, Sparse Loss: 0.0746\n",
      "Epoch 72/200, Total Loss: 0.1034, Recon Loss: 0.0292, Sparse Loss: 0.0741\n",
      "Epoch 73/200, Total Loss: 0.1024, Recon Loss: 0.0287, Sparse Loss: 0.0737\n",
      "Epoch 74/200, Total Loss: 0.1015, Recon Loss: 0.0282, Sparse Loss: 0.0733\n",
      "Epoch 75/200, Total Loss: 0.1006, Recon Loss: 0.0277, Sparse Loss: 0.0730\n",
      "Epoch 76/200, Total Loss: 0.0998, Recon Loss: 0.0272, Sparse Loss: 0.0726\n",
      "Epoch 77/200, Total Loss: 0.0989, Recon Loss: 0.0267, Sparse Loss: 0.0722\n",
      "Epoch 78/200, Total Loss: 0.0981, Recon Loss: 0.0263, Sparse Loss: 0.0718\n",
      "Epoch 79/200, Total Loss: 0.0973, Recon Loss: 0.0259, Sparse Loss: 0.0715\n",
      "Epoch 80/200, Total Loss: 0.0966, Recon Loss: 0.0255, Sparse Loss: 0.0711\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 81/200, Total Loss: 0.0958, Recon Loss: 0.0251, Sparse Loss: 0.0707\n",
      "Epoch 82/200, Total Loss: 0.0950, Recon Loss: 0.0247, Sparse Loss: 0.0704\n",
      "Epoch 83/200, Total Loss: 0.0943, Recon Loss: 0.0243, Sparse Loss: 0.0700\n",
      "Epoch 84/200, Total Loss: 0.0936, Recon Loss: 0.0239, Sparse Loss: 0.0697\n",
      "Epoch 85/200, Total Loss: 0.0929, Recon Loss: 0.0235, Sparse Loss: 0.0693\n",
      "Epoch 86/200, Total Loss: 0.0922, Recon Loss: 0.0232, Sparse Loss: 0.0690\n",
      "Epoch 87/200, Total Loss: 0.0915, Recon Loss: 0.0228, Sparse Loss: 0.0687\n",
      "Epoch 88/200, Total Loss: 0.0909, Recon Loss: 0.0225, Sparse Loss: 0.0684\n",
      "Epoch 89/200, Total Loss: 0.0902, Recon Loss: 0.0222, Sparse Loss: 0.0681\n",
      "Epoch 90/200, Total Loss: 0.0896, Recon Loss: 0.0218, Sparse Loss: 0.0678\n",
      "Epoch 91/200, Total Loss: 0.0890, Recon Loss: 0.0215, Sparse Loss: 0.0675\n",
      "Epoch 92/200, Total Loss: 0.0884, Recon Loss: 0.0212, Sparse Loss: 0.0672\n",
      "Epoch 93/200, Total Loss: 0.0877, Recon Loss: 0.0209, Sparse Loss: 0.0668\n",
      "Epoch 94/200, Total Loss: 0.0872, Recon Loss: 0.0206, Sparse Loss: 0.0665\n",
      "Epoch 95/200, Total Loss: 0.0866, Recon Loss: 0.0203, Sparse Loss: 0.0663\n",
      "Epoch 96/200, Total Loss: 0.0860, Recon Loss: 0.0200, Sparse Loss: 0.0660\n",
      "Epoch 97/200, Total Loss: 0.0854, Recon Loss: 0.0197, Sparse Loss: 0.0657\n",
      "Epoch 98/200, Total Loss: 0.0849, Recon Loss: 0.0195, Sparse Loss: 0.0654\n",
      "Epoch 99/200, Total Loss: 0.0843, Recon Loss: 0.0192, Sparse Loss: 0.0651\n",
      "Epoch 100/200, Total Loss: 0.0838, Recon Loss: 0.0189, Sparse Loss: 0.0649\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 101/200, Total Loss: 0.0832, Recon Loss: 0.0187, Sparse Loss: 0.0646\n",
      "Epoch 102/200, Total Loss: 0.0827, Recon Loss: 0.0184, Sparse Loss: 0.0643\n",
      "Epoch 103/200, Total Loss: 0.0822, Recon Loss: 0.0181, Sparse Loss: 0.0641\n",
      "Epoch 104/200, Total Loss: 0.0817, Recon Loss: 0.0179, Sparse Loss: 0.0638\n",
      "Epoch 105/200, Total Loss: 0.0812, Recon Loss: 0.0177, Sparse Loss: 0.0635\n",
      "Epoch 106/200, Total Loss: 0.0807, Recon Loss: 0.0174, Sparse Loss: 0.0633\n",
      "Epoch 107/200, Total Loss: 0.0802, Recon Loss: 0.0172, Sparse Loss: 0.0630\n",
      "Epoch 108/200, Total Loss: 0.0797, Recon Loss: 0.0170, Sparse Loss: 0.0628\n",
      "Epoch 109/200, Total Loss: 0.0792, Recon Loss: 0.0167, Sparse Loss: 0.0625\n",
      "Epoch 110/200, Total Loss: 0.0788, Recon Loss: 0.0165, Sparse Loss: 0.0623\n",
      "Epoch 111/200, Total Loss: 0.0783, Recon Loss: 0.0163, Sparse Loss: 0.0620\n",
      "Epoch 112/200, Total Loss: 0.0778, Recon Loss: 0.0161, Sparse Loss: 0.0618\n",
      "Epoch 113/200, Total Loss: 0.0774, Recon Loss: 0.0158, Sparse Loss: 0.0615\n",
      "Epoch 114/200, Total Loss: 0.0769, Recon Loss: 0.0156, Sparse Loss: 0.0613\n",
      "Epoch 115/200, Total Loss: 0.0765, Recon Loss: 0.0154, Sparse Loss: 0.0611\n",
      "Epoch 116/200, Total Loss: 0.0760, Recon Loss: 0.0152, Sparse Loss: 0.0608\n",
      "Epoch 117/200, Total Loss: 0.0756, Recon Loss: 0.0150, Sparse Loss: 0.0606\n",
      "Epoch 118/200, Total Loss: 0.0752, Recon Loss: 0.0148, Sparse Loss: 0.0603\n",
      "Epoch 119/200, Total Loss: 0.0747, Recon Loss: 0.0146, Sparse Loss: 0.0601\n",
      "Epoch 120/200, Total Loss: 0.0743, Recon Loss: 0.0144, Sparse Loss: 0.0599\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 121/200, Total Loss: 0.0739, Recon Loss: 0.0142, Sparse Loss: 0.0597\n",
      "Epoch 122/200, Total Loss: 0.0735, Recon Loss: 0.0140, Sparse Loss: 0.0595\n",
      "Epoch 123/200, Total Loss: 0.0731, Recon Loss: 0.0138, Sparse Loss: 0.0592\n",
      "Epoch 124/200, Total Loss: 0.0726, Recon Loss: 0.0136, Sparse Loss: 0.0590\n",
      "Epoch 125/200, Total Loss: 0.0722, Recon Loss: 0.0134, Sparse Loss: 0.0588\n",
      "Epoch 126/200, Total Loss: 0.0718, Recon Loss: 0.0133, Sparse Loss: 0.0586\n",
      "Epoch 127/200, Total Loss: 0.0715, Recon Loss: 0.0131, Sparse Loss: 0.0584\n",
      "Epoch 128/200, Total Loss: 0.0711, Recon Loss: 0.0129, Sparse Loss: 0.0582\n",
      "Epoch 129/200, Total Loss: 0.0707, Recon Loss: 0.0127, Sparse Loss: 0.0580\n",
      "Epoch 130/200, Total Loss: 0.0703, Recon Loss: 0.0125, Sparse Loss: 0.0577\n",
      "Epoch 131/200, Total Loss: 0.0699, Recon Loss: 0.0124, Sparse Loss: 0.0575\n",
      "Epoch 132/200, Total Loss: 0.0695, Recon Loss: 0.0122, Sparse Loss: 0.0573\n",
      "Epoch 133/200, Total Loss: 0.0692, Recon Loss: 0.0120, Sparse Loss: 0.0571\n",
      "Epoch 134/200, Total Loss: 0.0688, Recon Loss: 0.0119, Sparse Loss: 0.0569\n",
      "Epoch 135/200, Total Loss: 0.0685, Recon Loss: 0.0117, Sparse Loss: 0.0567\n",
      "Epoch 136/200, Total Loss: 0.0681, Recon Loss: 0.0116, Sparse Loss: 0.0565\n",
      "Epoch 137/200, Total Loss: 0.0677, Recon Loss: 0.0114, Sparse Loss: 0.0563\n",
      "Epoch 138/200, Total Loss: 0.0674, Recon Loss: 0.0112, Sparse Loss: 0.0562\n",
      "Epoch 139/200, Total Loss: 0.0670, Recon Loss: 0.0111, Sparse Loss: 0.0560\n",
      "Epoch 140/200, Total Loss: 0.0667, Recon Loss: 0.0109, Sparse Loss: 0.0558\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 141/200, Total Loss: 0.0664, Recon Loss: 0.0108, Sparse Loss: 0.0556\n",
      "Epoch 142/200, Total Loss: 0.0660, Recon Loss: 0.0106, Sparse Loss: 0.0554\n",
      "Epoch 143/200, Total Loss: 0.0657, Recon Loss: 0.0105, Sparse Loss: 0.0552\n",
      "Epoch 144/200, Total Loss: 0.0653, Recon Loss: 0.0104, Sparse Loss: 0.0550\n",
      "Epoch 145/200, Total Loss: 0.0650, Recon Loss: 0.0102, Sparse Loss: 0.0548\n",
      "Epoch 146/200, Total Loss: 0.0647, Recon Loss: 0.0101, Sparse Loss: 0.0546\n",
      "Epoch 147/200, Total Loss: 0.0644, Recon Loss: 0.0099, Sparse Loss: 0.0544\n",
      "Epoch 148/200, Total Loss: 0.0641, Recon Loss: 0.0098, Sparse Loss: 0.0542\n",
      "Epoch 149/200, Total Loss: 0.0637, Recon Loss: 0.0097, Sparse Loss: 0.0541\n",
      "Epoch 150/200, Total Loss: 0.0634, Recon Loss: 0.0096, Sparse Loss: 0.0539\n",
      "Epoch 151/200, Total Loss: 0.0631, Recon Loss: 0.0094, Sparse Loss: 0.0537\n",
      "Epoch 152/200, Total Loss: 0.0628, Recon Loss: 0.0093, Sparse Loss: 0.0535\n",
      "Epoch 153/200, Total Loss: 0.0625, Recon Loss: 0.0092, Sparse Loss: 0.0533\n",
      "Epoch 154/200, Total Loss: 0.0622, Recon Loss: 0.0091, Sparse Loss: 0.0532\n",
      "Epoch 155/200, Total Loss: 0.0619, Recon Loss: 0.0089, Sparse Loss: 0.0530\n",
      "Epoch 156/200, Total Loss: 0.0616, Recon Loss: 0.0088, Sparse Loss: 0.0528\n",
      "Epoch 157/200, Total Loss: 0.0613, Recon Loss: 0.0087, Sparse Loss: 0.0526\n",
      "Epoch 158/200, Total Loss: 0.0611, Recon Loss: 0.0086, Sparse Loss: 0.0525\n",
      "Epoch 159/200, Total Loss: 0.0608, Recon Loss: 0.0085, Sparse Loss: 0.0523\n",
      "Epoch 160/200, Total Loss: 0.0605, Recon Loss: 0.0084, Sparse Loss: 0.0521\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 161/200, Total Loss: 0.0602, Recon Loss: 0.0083, Sparse Loss: 0.0519\n",
      "Epoch 162/200, Total Loss: 0.0599, Recon Loss: 0.0082, Sparse Loss: 0.0518\n",
      "Epoch 163/200, Total Loss: 0.0597, Recon Loss: 0.0081, Sparse Loss: 0.0516\n",
      "Epoch 164/200, Total Loss: 0.0594, Recon Loss: 0.0080, Sparse Loss: 0.0514\n",
      "Epoch 165/200, Total Loss: 0.0591, Recon Loss: 0.0079, Sparse Loss: 0.0512\n",
      "Epoch 166/200, Total Loss: 0.0589, Recon Loss: 0.0078, Sparse Loss: 0.0511\n",
      "Epoch 167/200, Total Loss: 0.0586, Recon Loss: 0.0077, Sparse Loss: 0.0509\n",
      "Epoch 168/200, Total Loss: 0.0584, Recon Loss: 0.0076, Sparse Loss: 0.0507\n",
      "Epoch 169/200, Total Loss: 0.0581, Recon Loss: 0.0075, Sparse Loss: 0.0506\n",
      "Epoch 170/200, Total Loss: 0.0578, Recon Loss: 0.0074, Sparse Loss: 0.0504\n",
      "Epoch 171/200, Total Loss: 0.0576, Recon Loss: 0.0074, Sparse Loss: 0.0502\n",
      "Epoch 172/200, Total Loss: 0.0573, Recon Loss: 0.0073, Sparse Loss: 0.0501\n",
      "Epoch 173/200, Total Loss: 0.0571, Recon Loss: 0.0072, Sparse Loss: 0.0499\n",
      "Epoch 174/200, Total Loss: 0.0568, Recon Loss: 0.0071, Sparse Loss: 0.0497\n",
      "Epoch 175/200, Total Loss: 0.0566, Recon Loss: 0.0070, Sparse Loss: 0.0496\n",
      "Epoch 176/200, Total Loss: 0.0564, Recon Loss: 0.0070, Sparse Loss: 0.0494\n",
      "Epoch 177/200, Total Loss: 0.0561, Recon Loss: 0.0069, Sparse Loss: 0.0492\n",
      "Epoch 178/200, Total Loss: 0.0559, Recon Loss: 0.0068, Sparse Loss: 0.0491\n",
      "Epoch 179/200, Total Loss: 0.0556, Recon Loss: 0.0067, Sparse Loss: 0.0489\n",
      "Epoch 180/200, Total Loss: 0.0554, Recon Loss: 0.0067, Sparse Loss: 0.0487\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Epoch 181/200, Total Loss: 0.0552, Recon Loss: 0.0066, Sparse Loss: 0.0486\n",
      "Epoch 182/200, Total Loss: 0.0549, Recon Loss: 0.0065, Sparse Loss: 0.0484\n",
      "Epoch 183/200, Total Loss: 0.0547, Recon Loss: 0.0065, Sparse Loss: 0.0482\n",
      "Epoch 184/200, Total Loss: 0.0545, Recon Loss: 0.0064, Sparse Loss: 0.0481\n",
      "Epoch 185/200, Total Loss: 0.0543, Recon Loss: 0.0063, Sparse Loss: 0.0479\n",
      "Epoch 186/200, Total Loss: 0.0540, Recon Loss: 0.0063, Sparse Loss: 0.0478\n",
      "Epoch 187/200, Total Loss: 0.0538, Recon Loss: 0.0062, Sparse Loss: 0.0476\n",
      "Epoch 188/200, Total Loss: 0.0536, Recon Loss: 0.0061, Sparse Loss: 0.0474\n",
      "Epoch 189/200, Total Loss: 0.0534, Recon Loss: 0.0061, Sparse Loss: 0.0473\n",
      "Epoch 190/200, Total Loss: 0.0531, Recon Loss: 0.0060, Sparse Loss: 0.0471\n",
      "Epoch 191/200, Total Loss: 0.0529, Recon Loss: 0.0060, Sparse Loss: 0.0470\n",
      "Epoch 192/200, Total Loss: 0.0527, Recon Loss: 0.0059, Sparse Loss: 0.0468\n",
      "Epoch 193/200, Total Loss: 0.0525, Recon Loss: 0.0058, Sparse Loss: 0.0467\n",
      "Epoch 194/200, Total Loss: 0.0523, Recon Loss: 0.0058, Sparse Loss: 0.0465\n",
      "Epoch 195/200, Total Loss: 0.0521, Recon Loss: 0.0057, Sparse Loss: 0.0463\n",
      "Epoch 196/200, Total Loss: 0.0519, Recon Loss: 0.0057, Sparse Loss: 0.0462\n",
      "Epoch 197/200, Total Loss: 0.0517, Recon Loss: 0.0056, Sparse Loss: 0.0460\n",
      "Epoch 198/200, Total Loss: 0.0515, Recon Loss: 0.0056, Sparse Loss: 0.0459\n",
      "Epoch 199/200, Total Loss: 0.0513, Recon Loss: 0.0055, Sparse Loss: 0.0458\n",
      "Epoch 200/200, Total Loss: 0.0511, Recon Loss: 0.0055, Sparse Loss: 0.0456\n",
      "Evaluated features shape: torch.Size([135, 844])\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "data_loader = create_data_loader(activations, batch_size)\n",
    "sae_model, training_losses, resonctruction_losses, sparsity_losses, sae_feature_dim, input_dim = train_sparse_autoencoder(\n",
    "    activations, data_loader, num_epochs, sae_l1_coeff, llm_model_name=llm_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4017e8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_dict[\"A: Why are you carrying that wet umbrella? B: Because it started to rain on my way here, I had to use it.\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06047e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (encoder): Linear(in_features=768, out_features=844, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (decoder): Linear(in_features=844, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "sae_model.to(device)\n",
    "# sae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd2c006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAE Feature 0 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 1 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 2 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 3 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 4 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 2.3402\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 8)\n",
      "    文脈: text for testing **the** sparse auto ##en\n",
      "    元テキスト (一部): \"This is a sample text for testing the Sparse Autoencoder.\"\n",
      "  順位 2: 活性化値 = 0.7690\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.7521\n",
      "    トークン: '##r' (テキスト内の実トークンindex: 13)\n",
      "    文脈: auto ##en ##code **##r** . [SEP]\n",
      "    元テキスト (一部): \"This is a sample text for testing the Sparse Autoencoder.\"\n",
      "  順位 4: 活性化値 = 0.5998\n",
      "    トークン: '##ch' (テキスト内の実トークンindex: 4)\n",
      "    文脈: p ##yt ##or **##ch** is a widely\n",
      "    元テキスト (一部): \"PyTorch is a widely used deep learning framework.\"\n",
      "  順位 5: 活性化値 = 0.5401\n",
      "    トークン: 'in' (テキスト内の実トークンindex: 4)\n",
      "    文脈: cats enjoy sleeping **in** warm , sunny\n",
      "    元テキスト (一部): \"Cats enjoy sleeping in warm, sunny spots.\"\n",
      "\n",
      "--- SAE Feature 5 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 6 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 7 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 8 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 0.0000\n",
      "    トークン: '[CLS]' (テキスト内の実トークンindex: 0)\n",
      "    文脈: **[CLS]** the quick brown\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 2: 活性化値 = 0.0000\n",
      "    トークン: 'the' (テキスト内の実トークンindex: 1)\n",
      "    文脈: [CLS] **the** quick brown fox\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 3: 活性化値 = 0.0000\n",
      "    トークン: 'quick' (テキスト内の実トークンindex: 2)\n",
      "    文脈: [CLS] the **quick** brown fox jumps\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 4: 活性化値 = 0.0000\n",
      "    トークン: 'brown' (テキスト内の実トークンindex: 3)\n",
      "    文脈: [CLS] the quick **brown** fox jumps over\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "  順位 5: 活性化値 = 0.0000\n",
      "    トークン: 'fox' (テキスト内の実トークンindex: 4)\n",
      "    文脈: the quick brown **fox** jumps over the\n",
      "    元テキスト (一部): \"The quick brown fox jumps over the lazy dog.\"\n",
      "\n",
      "--- SAE Feature 9 を最も強く活性化するトークン\n",
      "  順位 1: 活性化値 = 8.4731\n",
      "    トークン: 'deep' (テキスト内の実トークンindex: 9)\n",
      "    文脈: a widely used **deep** learning framework .\n",
      "    元テキスト (一部): \"PyTorch is a widely used deep learning framework.\"\n",
      "  順位 2: 活性化値 = 0.2032\n",
      "    トークン: '[SEP]' (テキスト内の実トークンindex: 11)\n",
      "    文脈: generate text . **[SEP]**\n",
      "    元テキスト (一部): \"A large language model can process and generate text.\"\n",
      "  順位 3: 活性化値 = 0.2016\n",
      "    トークン: '[SEP]' (テキスト内の実トークンindex: 10)\n",
      "    文脈: sunny spots . **[SEP]**\n",
      "    元テキスト (一部): \"Cats enjoy sleeping in warm, sunny spots.\"\n",
      "  順位 4: 活性化値 = 0.2011\n",
      "    トークン: '[SEP]' (テキスト内の実トークンindex: 13)\n",
      "    文脈: learning framework . **[SEP]**\n",
      "    元テキスト (一部): \"PyTorch is a widely used deep learning framework.\"\n",
      "  順位 5: 活性化値 = 0.2001\n",
      "    トークン: '[SEP]' (テキスト内の実トークンindex: 15)\n",
      "    文脈: ##code ##r . **[SEP]**\n",
      "    元テキスト (一部): \"This is a sample text for testing the Sparse Autoencoder.\"\n"
     ]
    }
   ],
   "source": [
    "token_info_list = []\n",
    "all_sae_features_list = []\n",
    "global_token_idx = 0\n",
    "\n",
    "# トークナイザーにpad_tokenが設定されているか確認 (活性化抽出時と条件を合わせるため)\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"トークナイザーのpad_tokenをeos_token ({tokenizer.pad_token}) に設定しました。\")\n",
    "    else:\n",
    "        # これはデモスクリプト (demo_train_sae_gpt.py, demo_sae_train.py) や\n",
    "        # activation_utils.py 内の処理と整合性を取る必要があります。\n",
    "        print(\"警告: トークナイザーにpad_tokenが設定されていません。'[PAD]'を追加します。\")\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        # LLMモデルの埋め込み層のリサイズが必要な場合がある点に注意 (model.resize_token_embeddings(len(tokenizer)))\n",
    "\n",
    "if not activations_dict:\n",
    "    print(\"活性化辞書 (activations_dict) が空です。特徴分析をスキップします。\")\n",
    "else:\n",
    "   for original_text, token_activations in activations_dict.items():\n",
    "      # 活性ベクトルをGPUに転送\n",
    "      token_activations = token_activations.to(device)\n",
    "      \n",
    "      # 学習済みのSAEモデルを使用して、トークンの活性化をエンコード\n",
    "      with torch.no_grad():\n",
    "         sae_model_pre_relu = sae_model.encoder(token_activations)\n",
    "         sae_features_for_text = sae_model.relu(sae_model_pre_relu)\n",
    "\n",
    "      all_sae_features_list.append(sae_features_for_text.cpu())\n",
    "\n",
    "      # トークンを取得\n",
    "      inputs = tokenizer(original_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "      \n",
    "      # パディングトークンを除外するためにAttention Maskを使用\n",
    "      attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "      input_ids_squeeze = inputs[\"input_ids\"].squeeze(0)\n",
    "\n",
    "      # トークンIDを取得\n",
    "      actual_tokens_ids_for_text = input_ids_squeeze[attention_mask == 1]\n",
    "      actual_tokens_str_list = tokenizer.convert_ids_to_tokens(actual_tokens_ids_for_text)\n",
    "\n",
    "      if len(actual_tokens_str_list) != sae_features_for_text.shape[0]:\n",
    "         print(f\"Warning: Mismatch in token count for text: {original_text}\")\n",
    "         continue\n",
    "      \n",
    "      for token_idx_in_text in range(sae_features_for_text.shape[0]):\n",
    "         token_info_list.append({\n",
    "            \"original_text\": original_text,\n",
    "            \"token_idx_in_text\": token_idx_in_text,   # テキスト内の実施あのトークンに対するインデックス\n",
    "            \"token_str\": actual_tokens_str_list[token_idx_in_text],  # トークンの文字列表現\n",
    "            \"global_token_idx\": global_token_idx,  # データセット全体を通したトークンのインデックス\n",
    "         })\n",
    "         \n",
    "         global_token_idx += 1\n",
    "         \n",
    "# 以下結果の表示\n",
    "if not all_sae_features_list:\n",
    "   print(\"No SAE features found for the token.\")\n",
    "\n",
    "else:\n",
    "   concatenated_sae_features = torch.cat(all_sae_features_list, dim=0)  # Shape: (num_tokens, sae_feature_dim)\n",
    "   sae_total_features = concatenated_sae_features.shape[0]\n",
    "   \n",
    "   num_sae_features_to_analyze = min(10, sae_total_features) # 最大10個のSAE特徴を分析\n",
    "   num_top_tokens_per_feature = 5  # 各SAE特徴に対して上位5つのトークンを分析\n",
    "\n",
    "   # 指定した数のSAE特徴を分析するためのループ\n",
    "   for feature_idx_to_analyze in range(num_sae_features_to_analyze):\n",
    "      # 現在のSAE特徴次元に対応する前トークンの活性を取得\n",
    "      feature_column_activation = concatenated_sae_features[:, feature_idx_to_analyze]\n",
    "      \n",
    "      # 上位k個の活性化とそのグローバルインデックスを取得\n",
    "      actual_k = min(num_top_tokens_per_feature, len(feature_column_activation))\n",
    "      if actual_k == 0 : continue\n",
    "      \n",
    "      top_k_values, top_k_global_indices = torch.topk(feature_column_activation, k=actual_k)\n",
    "      \n",
    "      print(f\"\\n--- SAE Feature {feature_idx_to_analyze} を最も強く活性化するトークン\")\n",
    "      \n",
    "      if top_k_values.numel() == 0:\n",
    "         print(\"No top tokens found for this feature.\")\n",
    "         continue\n",
    "      \n",
    "      # 上位k個のトークンの情報を表示\n",
    "      for rank, (activation_value, global_token_idx_item) in enumerate(zip(top_k_values, top_k_global_indices)):\n",
    "         global_idx = global_token_idx_item.item()    # テンソルから値を取り出す\n",
    "         if global_idx < len(token_info_list):        \n",
    "            token_info = token_info_list[global_idx]  # 取得したトークン情報\n",
    "            \n",
    "            text_snippet = token_info[\"original_text\"]   # 元のテキスト\n",
    "            \n",
    "            # 文脈表示のために、元のテキストを再度トークナイズ(表示用)\n",
    "            inputs_ctx = tokenizer(text_snippet,\n",
    "                                 return_tensors=\"pt\",\n",
    "                                 truncation=True,\n",
    "                                 max_length=128,\n",
    "                                 padding=\"max_length\",\n",
    "                                 return_attention_mask=True)\n",
    "            ids_ctx = inputs_ctx[\"input_ids\"].squeeze()[inputs_ctx[\"attention_mask\"].squeeze() == 1]\n",
    "            tokens_ctx = tokenizer.convert_ids_to_tokens(ids_ctx)\n",
    "            \n",
    "            # 上記 tokens_ctx リスト内でのインデックスに相当\n",
    "            tok_idx_in_ctx = token_info[\"token_idx_in_text\"]\n",
    "            \n",
    "            context_window_size = 3    # 表示する前後のトークン数\n",
    "            start_idx = max(0, tok_idx_in_ctx - context_window_size)\n",
    "            end_idx = min(len(tokens_ctx), tok_idx_in_ctx + context_window_size + 1)\n",
    "            \n",
    "            context_display_parts = []                  \n",
    "            for i in range(start_idx, end_idx):\n",
    "               if i == tok_idx_in_ctx:\n",
    "                  context_display_parts.append(f\"**{tokens_ctx[i]}**\")\n",
    "               else:\n",
    "                  context_display_parts.append(tokens_ctx[i])\n",
    "            context_str = \" \".join(context_display_parts)\n",
    "\n",
    "            print(f\"  順位 {rank + 1}: 活性化値 = {activation_value.item():.4f}\")\n",
    "            print(f\"    トークン: '{token_info['token_str']}' (テキスト内の実トークンindex: {tok_idx_in_ctx})\")\n",
    "            print(f\"    文脈: {context_str}\")\n",
    "            text_preview = (text_snippet[:70] + '...') if len(text_snippet) > 70 else text_snippet # テキストのプレビュー\n",
    "            print(f\"    元テキスト (一部): \\\"{text_preview}\\\"\")\n",
    "         else:\n",
    "            print(f\"  順位 {rank + 1}: エラー - グローバルインデックス {global_idx} が範囲外です。\")                  \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
