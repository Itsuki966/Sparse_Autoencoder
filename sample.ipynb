{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462cc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sae_model import SparseAutoencoder\n",
    "from activation_utils import get_llm_activations_residual_stream\n",
    "from sae_trainer import extract_activations, create_data_loader, train_sparse_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_activationsの変数設定\n",
    "llm_model_name = \"distilgpt2\"    # 使用するLLMモデル名    \n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A large language model can process and generate text.\",\n",
    "    \"Cats enjoy sleeping in warm, sunny spots.\",\n",
    "    \"This is a sample text for testing the Sparse Autoencoder.\",\n",
    "    \"PyTorch is a widely used deep learning framework.\" ,\n",
    "    ]  # 使用するテキスト\n",
    "target_layer_idx = 5    # 抽出するLLMの層インデックス\n",
    "num_samples_for_training = 5  # 訓練に使用するサンプル数\n",
    "\n",
    "# create_data_loaderの変数設定\n",
    "batch_size = 256  # バッチサイズ\n",
    "\n",
    "# train_sparse_autoencoderの変数設定\n",
    "num_epochs = 200  # 訓練エポック数\n",
    "sae_l1_coeff = 1e-4 # スパース性の度合いを調整する係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7240cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModel.from_pretrained(llm_model_name)\n",
    "training_texts = [texts[i] for i in range(num_samples_for_training)]\n",
    "\n",
    "activations, activations_dict = get_llm_activations_residual_stream(\n",
    "    llm_model, tokenizer, training_texts, target_layer_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_data_loader(activations, batch_size)\n",
    "sae_model, training_losses, resonctruction_losses, sparsity_losses, sae_feature_dim, input_dim = train_sparse_autoencoder(\n",
    "    activations, data_loader, num_epochs, sae_l1_coeff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_dict[\"This is a sample text for testing the Sparse Autoencoder.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06047e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "sae_model.to(device)\n",
    "# sae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_info_list = []\n",
    "all_sae_features_list = []\n",
    "global_token_idx = 0\n",
    "\n",
    "# トークナイザーにpad_tokenが設定されているか確認 (活性化抽出時と条件を合わせるため)\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"トークナイザーのpad_tokenをeos_token ({tokenizer.pad_token}) に設定しました。\")\n",
    "    else:\n",
    "        # これはデモスクリプト (demo_train_sae_gpt.py, demo_sae_train.py) や\n",
    "        # activation_utils.py 内の処理と整合性を取る必要があります。\n",
    "        print(\"警告: トークナイザーにpad_tokenが設定されていません。'[PAD]'を追加します。\")\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        # LLMモデルの埋め込み層のリサイズが必要な場合がある点に注意 (model.resize_token_embeddings(len(tokenizer)))\n",
    "\n",
    "if not activations_dict:\n",
    "    print(\"活性化辞書 (activations_dict) が空です。特徴分析をスキップします。\")\n",
    "else:\n",
    "   for original_text, token_activations in activations_dict.items():\n",
    "      # 活性ベクトルをGPUに転送\n",
    "      token_activations = token_activations.to(device)\n",
    "      \n",
    "      # 学習済みのSAEモデルを使用して、トークンの活性化をエンコード\n",
    "      with torch.no_grad():\n",
    "         sae_model_pre_relu = sae_model.encoder(token_activations)\n",
    "         sae_features_for_text = sae_model.relu(sae_model_pre_relu)\n",
    "\n",
    "      all_sae_features_list.append(sae_features_for_text.cpu())\n",
    "\n",
    "      # トークンを取得\n",
    "      inputs = tokenizer(original_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "      \n",
    "      # パディングトークンを除外するためにAttention Maskを使用\n",
    "      attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "      input_ids_squeeze = inputs[\"input_ids\"].squeeze(0)\n",
    "\n",
    "      # トークンIDを取得\n",
    "      actual_tokens_ids_for_text = input_ids_squeeze[attention_mask == 1]\n",
    "      actual_tokens_str_list = tokenizer.convert_ids_to_tokens(actual_tokens_ids_for_text)\n",
    "\n",
    "      if len(actual_tokens_str_list) != sae_features_for_text.shape[0]:\n",
    "         print(f\"Warning: Mismatch in token count for text: {original_text}\")\n",
    "         continue\n",
    "      \n",
    "      for token_idx_in_text in range(sae_features_for_text.shape[0]):\n",
    "         token_info_list.append({\n",
    "            \"original_text\": original_text,\n",
    "            \"token_idx_in_text\": token_idx_in_text,   # テキスト内の実施あのトークンに対するインデックス\n",
    "            \"token_str\": actual_tokens_str_list[token_idx_in_text],  # トークンの文字列表現\n",
    "            \"global_token_idx\": global_token_idx,  # データセット全体を通したトークンのインデックス\n",
    "         })\n",
    "         \n",
    "         global_token_idx += 1\n",
    "         \n",
    "# 以下結果の表示\n",
    "if not all_sae_features_list:\n",
    "   print(\"No SAE features found for the token.\")\n",
    "\n",
    "else:\n",
    "   concatenated_sae_features = torch.cat(all_sae_features_list, dim=0)  # Shape: (num_tokens, sae_feature_dim)\n",
    "   sae_total_features = concatenated_sae_features.shape[0]\n",
    "   \n",
    "   num_sae_features_to_analyze = min(10, sae_total_features) # 最大10個のSAE特徴を分析\n",
    "   num_top_tokens_per_feature = 5  # 各SAE特徴に対して上位5つのトークンを分析\n",
    "\n",
    "   # 指定した数のSAE特徴を分析するためのループ\n",
    "   for feature_idx_to_analyze in range(num_sae_features_to_analyze):\n",
    "      # 現在のSAE特徴次元に対応する前トークンの活性を取得\n",
    "      feature_column_activation = concatenated_sae_features[:, feature_idx_to_analyze]\n",
    "      \n",
    "      # 上位k個の活性化とそのグローバルインデックスを取得\n",
    "      actual_k = min(num_top_tokens_per_feature, len(feature_column_activation))\n",
    "      if actual_k == 0 : continue\n",
    "      \n",
    "      top_k_values, top_k_global_indices = torch.topk(feature_column_activation, k=actual_k)\n",
    "      \n",
    "      print(f\"\\n--- SAE Feature {feature_idx_to_analyze} を最も強く活性化するトークン\")\n",
    "      \n",
    "      if top_k_values.numel() == 0:\n",
    "         print(\"No top tokens found for this feature.\")\n",
    "         continue\n",
    "      \n",
    "      # 上位k個のトークンの情報を表示\n",
    "      for rank, (activation_value, global_token_idx_item) in enumerate(zip(top_k_values, top_k_global_indices)):\n",
    "         global_idx = global_token_idx_item.item()    # テンソルから値を取り出す\n",
    "         if global_idx < len(token_info_list):        \n",
    "            token_info = token_info_list[global_idx]  # 取得したトークン情報\n",
    "            \n",
    "            text_snippet = token_info[\"original_text\"]   # 元のテキスト\n",
    "            \n",
    "            # 文脈表示のために、元のテキストを再度トークナイズ(表示用)\n",
    "            inputs_ctx = tokenizer(text_snippet,\n",
    "                                 return_tensors=\"pt\",\n",
    "                                 truncation=True,\n",
    "                                 max_length=128,\n",
    "                                 padding=\"max_length\",\n",
    "                                 return_attention_mask=True)\n",
    "            ids_ctx = inputs_ctx[\"input_ids\"].squeeze()[inputs_ctx[\"attention_mask\"].squeeze() == 1]\n",
    "            tokens_ctx = tokenizer.convert_ids_to_tokens(ids_ctx)\n",
    "            \n",
    "            # 上記 tokens_ctx リスト内でのインデックスに相当\n",
    "            tok_idx_in_ctx = token_info[\"token_idx_in_text\"]\n",
    "            \n",
    "            context_window_size = 3    # 表示する前後のトークン数\n",
    "            start_idx = max(0, tok_idx_in_ctx - context_window_size)\n",
    "            end_idx = min(len(tokens_ctx), tok_idx_in_ctx + context_window_size + 1)\n",
    "            \n",
    "            context_display_parts = []                  \n",
    "            for i in range(start_idx, end_idx):\n",
    "               if i == tok_idx_in_ctx:\n",
    "                  context_display_parts.append(f\"**{tokens_ctx[i]}**\")\n",
    "               else:\n",
    "                  context_display_parts.append(tokens_ctx[i])\n",
    "            context_str = \" \".join(context_display_parts)\n",
    "\n",
    "            print(f\"  順位 {rank + 1}: 活性化値 = {activation_value.item():.4f}\")\n",
    "            print(f\"    トークン: '{token_info['token_str']}' (テキスト内の実トークンindex: {tok_idx_in_ctx})\")\n",
    "            print(f\"    文脈: {context_str}\")\n",
    "            text_preview = (text_snippet[:70] + '...') if len(text_snippet) > 70 else text_snippet # テキストのプレビュー\n",
    "            print(f\"    元テキスト (一部): \\\"{text_preview}\\\"\")\n",
    "         else:\n",
    "            print(f\"  順位 {rank + 1}: エラー - グローバルインデックス {global_idx} が範囲外です。\")                  \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
